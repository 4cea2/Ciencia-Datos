{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH0dQxsYMTVR"
      },
      "source": [
        "# **Trabajo practico 1 - Checkpoint 4**\n",
        "**Grupo**: 26\n",
        "\n",
        "**Integrantes**:\n",
        "\n",
        "Garcia, Nicolas\n",
        "\n",
        "Vallcorba, Agustin\n",
        "\n",
        "Carbajal Robles, Kevin Emir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMR17Uglqd2I"
      },
      "source": [
        "## **Librerias y carga del data set**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.12.0\n",
        "!pip uninstall tensorflow -y\n",
        "!pip install tensorflow==2.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXmWEG--CUU2",
        "outputId": "f3b865ed-74e7-45eb-dc02-4cb832cf9392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.12.0\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.14.0\n",
            "    Uninstalling keras-2.14.0:\n",
            "      Successfully uninstalled keras-2.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.14.0 requires keras<2.15,>=2.14.0, but you have keras 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.12.0\n",
            "Found existing installation: tensorflow 2.14.0\n",
            "Uninstalling tensorflow-2.14.0:\n",
            "  Successfully uninstalled tensorflow-2.14.0\n",
            "Collecting tensorflow==2.12.0\n",
            "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (23.5.26)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.59.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.4.16)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.16.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0) (0.34.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.41.2)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.11.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Installing collected packages: tensorflow-estimator, gast, tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.14.0\n",
            "    Uninstalling tensorflow-estimator-2.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.14.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.4\n",
            "    Uninstalling gast-0.5.4:\n",
            "      Successfully uninstalled gast-0.5.4\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.14.1\n",
            "    Uninstalling tensorboard-2.14.1:\n",
            "      Successfully uninstalled tensorboard-2.14.1\n",
            "Successfully installed gast-0.4.0 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8RfiDmslZ4D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score,f1_score\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, KFold,RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn import preprocessing\n",
        "\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY2QQDN3NMal"
      },
      "source": [
        "Cargamos el dataset de train y test, que es el que obtuvimos en el checkpoint 2.\n",
        "\n",
        "De igual manera lo haremos para el dataset de test y el de train original, tambien el obtenido del chp1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlfnWaJmNCJ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "a470dbd0-e223-4ae1-ba2d-b5eac511943f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7829891517e5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Dataset de entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_data/ds_train_limpio.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Dataset de test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample_data/ds_test_limpio.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_data/ds_train_limpio.csv'"
          ]
        }
      ],
      "source": [
        "#Dataset de entrenamiento\n",
        "ds_train = pd.read_csv('sample_data/ds_train_limpio.csv')\n",
        "\n",
        "#Dataset de test\n",
        "ds_test = pd.read_csv(\"sample_data/ds_test_limpio.csv\")\n",
        "\n",
        "\n",
        "#Dataset de test\n",
        "ds_test_orig = pd.read_csv(\"sample_data/hotels_test.csv\")\n",
        "\n",
        "#Dataset de entrenamiento chp1\n",
        "ds_train_chp1 = pd.read_csv('sample_data/ds_train_chp1.csv')\n",
        "\n",
        "#Dataset de entrenamiento original\n",
        "ds_train_orig = pd.read_csv(\"https://drive.google.com/u/0/uc?id=1tjlunFxE63XIpYjWk8SPm2XPxixu8SW4&export=download\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brEYfic17Eel"
      },
      "source": [
        "## **Modelos**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9N-2mub-IL1"
      },
      "source": [
        "Para entrenar cada modelo, haremos una division del dataset de train.\n",
        "\n",
        "Entrenaremos con un 70%, y dejaremos un 30% para testear, al igual que el checkpoint 2 y 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6bxlk3r9oqr"
      },
      "outputs": [],
      "source": [
        "#Creo un dataset con los features que voy a usar para clasificar\n",
        "ds_train_x=ds_train.drop(['is_canceled'], axis='columns', inplace=False)\n",
        "\n",
        "#Creo un dataset con la variable target\n",
        "ds_train_y = ds_train['is_canceled'].copy()\n",
        "\n",
        "#Genero los conjuntos de train y test\n",
        "x_train, x_test, y_train, y_test = train_test_split(ds_train_x,\n",
        "                                                    ds_train_y,\n",
        "                                                    test_size=0.3,  #proporcion 70/30\n",
        "                                                    random_state=2) #semilla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEnbOCrL_L0t"
      },
      "source": [
        "En todos los modelos, evaluaremos su performance, explicaremos todas las métricas y mostraremos su matriz de confusion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO5N0PfWbcjS"
      },
      "source": [
        "Definiremos una funcion para calcular las metricas y su matriz de confusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1enFq--dKFK"
      },
      "outputs": [],
      "source": [
        "def metricas_y_matriz_confusion(y_prediccion, y_testeo):\n",
        "  #Calculo las métricas en el conjunto de evaluación\n",
        "  accuracy=accuracy_score(y_testeo,y_prediccion)\n",
        "  recall=recall_score(y_testeo,y_prediccion)\n",
        "  f1=f1_score(y_testeo,y_prediccion,)\n",
        "  precision=precision_score(y_testeo,y_prediccion)\n",
        "  cm = confusion_matrix(y_testeo, y_prediccion)\n",
        "\n",
        "  print(\"Accuracy: \"+str(accuracy))\n",
        "  print(\"Recall: \"+str(recall))\n",
        "  print(\"Precision: \"+str(precision))\n",
        "  print(\"f1 score: \"+str(f1))\n",
        "\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "  plt.xlabel('Predicción')\n",
        "  plt.ylabel('Valor Real')\n",
        "  plt.title('Matriz de Confusión')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modificaciones de datos (OPCIONAL)**"
      ],
      "metadata": {
        "id": "luk2h2_vDeSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aca pondriamos alguna que otra modificacion que hariamos en caso de que se nos ocurriesen otras que no habiamos pensado.\n",
        "\n",
        "En caso de que no usemos esta seccion, hay que borrarla"
      ],
      "metadata": {
        "id": "MHRva1oPDmqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Armo una version escalada con minmax (media=0 , var=1  )\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "x_train_minmax = min_max_scaler.fit_transform(x_train)\n",
        "x_test_minmax  = min_max_scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "j-Ib33yfDmqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Armo una version estandarizada\n",
        "stand_scaler = preprocessing.StandardScaler()\n",
        "x_train_norm = stand_scaler.fit_transform(x_train)\n",
        "x_test_norm = stand_scaler.transform(x_test)\n"
      ],
      "metadata": {
        "id": "ukzSijC1Dmqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Armo una version escalada con minmax (media=0 , var=1  )\n",
        "ds_test_minmax = min_max_scaler.fit_transform(ds_test)"
      ],
      "metadata": {
        "id": "Qw9qFWZNzDOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_test_norm=stand_scaler.transform(ds_test)"
      ],
      "metadata": {
        "id": "xO26lKhgVBe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f1_metric(y_true, y_pred):\n",
        "    f1 = tf.py_function(f1_score, (y_true, tf.round(y_pred)), tf.float32)\n",
        "    return f1"
      ],
      "metadata": {
        "id": "yeLXzGXq-btc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkOOdSQ67L3v"
      },
      "source": [
        "### **Modelo 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Red neuronal basica, en donde usaremos los datos transformados, ya sean de **MinMax** o **Normalizados**."
      ],
      "metadata": {
        "id": "W-mSvDi4oyVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calcula la cantidad de clases\n",
        "cant_clases=len(np.unique(y_train))\n",
        "d_in=(x_train_minmax.shape[1])\n",
        "\n",
        "modelo = keras.Sequential([\n",
        "    keras.layers.Dense(64, input_shape=(d_in,), activation='relu'),  # Capa densa con 64 unidades y función de activación ReLU\n",
        "    keras.layers.Dense(32, activation='relu'),  # Capa densa con 32 unidades y función de activación ReLU\n",
        "    keras.layers.Dense(16, activation='relu'),  # Capa densa con 32 unidades y función de activación ReLU\n",
        "    keras.layers.Dense(8, activation='relu'),  # Capa densa con 32 unidades y función de activación ReLU\n",
        "    keras.layers.Dense(1, activation='sigmoid')  # Capa de salida con función de activación sigmoid\n",
        "    ])\n",
        "\n",
        "modelo.summary()"
      ],
      "metadata": {
        "id": "wvndiivuDw_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo.compile(\n",
        "              optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
        "              loss='binary_crossentropy',\n",
        "  # metricas para ir calculando en cada iteracion o batch\n",
        "              metrics=[f1_metric],\n",
        ")\n",
        "\n",
        "cant_epochs=100\n",
        "modelo_entrenando = modelo.fit(x_train_minmax,y_train,\n",
        "                               epochs=cant_epochs,\n",
        "                               batch_size=50,\n",
        "                               verbose=False)"
      ],
      "metadata": {
        "id": "giA7NZgt-Y2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predic_ej1 = modelo.predict(x_test_minmax)\n",
        "y_predic_cat_ej1 = np.where(y_predic_ej1>0.4,1,0)\n",
        "\n",
        "ds_validacion=pd.DataFrame(y_predic_cat_ej1,y_test).reset_index()\n",
        "ds_validacion.columns=['y_pred','y_real']\n",
        "\n",
        "metricas_y_matriz_confusion(ds_validacion['y_pred'], ds_validacion['y_real'])"
      ],
      "metadata": {
        "id": "YZk1O52-MLYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7hZMqMw7OaQ"
      },
      "source": [
        "### **Modelo 2**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cant_clases = 1\n",
        "d_in = x_train_minmax.shape[1]\n",
        "\n",
        "modelo2 = keras.Sequential([\n",
        "    keras.layers.Dense(128, input_shape=(d_in,), activation='relu'),  # Capa densa con 128 unidades y función de activación ReLU\n",
        "    keras.layers.Dropout(0.2),  # Capa de dropout con 20% de dropout\n",
        "    keras.layers.Dense(64, activation='relu'),  # Capa densa con 64 unidades y función de activación ReLU\n",
        "    keras.layers.Dropout(0.2),  # Capa de dropout con 20% de dropout\n",
        "    keras.layers.Dense(32, activation='relu'),  # Capa densa con 32 unidades y función de activación ReLU\n",
        "    keras.layers.Dense(1, activation='sigmoid')  # Capa de salida con función de activación sigmoid\n",
        "])\n",
        "\n",
        "modelo2.summary()"
      ],
      "metadata": {
        "id": "spOPrJrvrZhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo2.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Optimizador Adam con learning rate 0.001\n",
        "    loss='binary_crossentropy',  # Pérdida para clasificación binaria\n",
        "    metrics=['accuracy']  # Métrica de precisión\n",
        ")\n",
        "\n",
        "cant_epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "modelo2_entrenando = modelo2.fit(x_train_minmax, y_train, epochs=cant_epochs, batch_size=batch_size, validation_data=(x_test_minmax, y_test))"
      ],
      "metadata": {
        "id": "tG6d4F7QrhYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predic_ej2 = modelo2.predict(x_test_minmax)\n",
        "y_predic_cat_ej2 = np.where(y_predic_ej2>0.5,1,0)\n",
        "\n",
        "ds_validacion2=pd.DataFrame(y_predic_cat_ej2,y_test).reset_index()\n",
        "ds_validacion2.columns=['y_pred','y_real']\n",
        "\n",
        "metricas_y_matriz_confusion(ds_validacion2['y_pred'], ds_validacion2['y_real'])"
      ],
      "metadata": {
        "id": "c8xhYlfaQoR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSM1Scda7R3N"
      },
      "source": [
        "### **Modelo 3**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cant_clases = 1\n",
        "d_in = x_train_minmax.shape[1]\n",
        "\n",
        "modelo3 = keras.Sequential([\n",
        "    keras.layers.Dense(256, input_shape=(d_in,), activation='tanh'),  # Capa densa con 256 unidades y función de activación tanh\n",
        "    keras.layers.Dropout(0.3),  # Capa de dropout con 30% de dropout\n",
        "    keras.layers.Dense(128, activation='tanh'),  # Capa densa con 128 unidades y función de activación tanh\n",
        "    keras.layers.Dropout(0.3),  # Capa de dropout con 30% de dropout\n",
        "    keras.layers.Dense(64, activation='tanh'),  # Capa densa con 64 unidades y función de activación tanh\n",
        "    keras.layers.Dense(1, activation='sigmoid')  # Capa de salida con función de activación sigmoid\n",
        "])\n",
        "\n",
        "modelo3.summary()"
      ],
      "metadata": {
        "id": "Z90LY3IDQosD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo3.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),  # Optimizador Adam con learning rate 0.001\n",
        "    loss='binary_crossentropy',  # Pérdida para clasificación binaria\n",
        "    metrics=['accuracy']  # Métrica de precisión\n",
        ")\n",
        "\n",
        "cant_epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "modelo3_entrenando = modelo3.fit(x_train_minmax, y_train, epochs=cant_epochs, batch_size=batch_size, validation_data=(x_test_minmax, y_test))"
      ],
      "metadata": {
        "id": "qSJObPgTHDL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predic_ej3 = modelo3.predict(x_test_minmax)\n",
        "y_predic_cat_ej3 = np.where(y_predic_ej3>0.5,1,0)\n",
        "\n",
        "ds_validacion3=pd.DataFrame(y_predic_cat_ej3,y_test).reset_index()\n",
        "ds_validacion3.columns=['y_pred','y_real']\n",
        "\n",
        "metricas_y_matriz_confusion(ds_validacion3['y_pred'], ds_validacion3['y_real'])"
      ],
      "metadata": {
        "id": "4YU_h3zkHDuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhDBGrcg_wHi"
      },
      "source": [
        "## **Prediccion final**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZNyetB5a1AL"
      },
      "source": [
        "Seleccionamos el dataset a predecir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkAzNnABa4iR"
      },
      "outputs": [],
      "source": [
        "x_submission = de_test_minmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mboSwi_8Basb"
      },
      "source": [
        "Luego de entrenar estos modelos con el dataset de train, vamos a predecir con el test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsjsClOvBoZL"
      },
      "outputs": [],
      "source": [
        "y_pred_final = modelo3.predict(x_submission)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_final_2d = y_pred_final.flatten()"
      ],
      "metadata": {
        "id": "ccCwGwCo15ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_final_final = np.where(y_pred_final_2d>0.5,1,0)"
      ],
      "metadata": {
        "id": "_6Mfw-GU2wlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s63C_7K6ByAb"
      },
      "source": [
        "## **Submission**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWruH0jQB1it"
      },
      "source": [
        "Haremos la exportacion de esta prediccion para subirla a la competencia de **kaggle**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LJd6A_pCFrb"
      },
      "source": [
        "Cargamos el dataset final con las predicciones hechas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JVJDZ5iB8uB"
      },
      "outputs": [],
      "source": [
        "df_submissing = pd.DataFrame({'id': ds_test_orig['id'], 'is_canceled': y_pred_final_final.astype(int)})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_submissing.head()"
      ],
      "metadata": {
        "id": "zwZKIKDi2hUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72y3mOCqCAvV"
      },
      "source": [
        "Finalmente, exportamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8pNdsQuCAaS"
      },
      "outputs": [],
      "source": [
        "df_submissing.to_csv('./submission_chp4_4.csv', index=False)"
      ]
    }
  ]
}